# -*- coding: utf-8 -*-
"""HackVerse

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oPhCxMIPmgHKccphlZ7A0QuJ8GwjQsIi
"""

import pandas as pd

!unzip /content/HackVerse.zip

train_df = pd.read_csv('/content/HackVerse/train_qWM28Yl.csv') 
test_df = pd.read_csv('/content/HackVerse/test_zo1G9sv.csv')

print(train_df.shape)
print(test_df.shape)

pd.set_option('display.max_columns',None)
train_df.head()

test_df.head()

# Check for datatype and nulls
train_df.info()

test_df.info()

train_df.isnull().sum()

test_df.isnull().sum()

print(train_df.shape)
train_df = train_df.drop_duplicates(keep='last')
print(train_df.shape)

"""No duplicates"""

print(test_df.shape)
test_df = test_df.drop_duplicates(keep='last')
print(test_df.shape)

train_df.describe(include='object')

test_df.describe(include='object')

# Drop Id column in train df (unique identifier and recheck for duplicates)
train_df = train_df.drop(columns=['policy_id'],axis=1)
print(train_df.shape)
train_df = train_df.drop_duplicates(keep='last')
print(train_df.shape)

"""There are no duplicates after removing unique identifier column (policy_id)"""

# Display the dataframe
train_df.head()

"""So far....
1. Datatypes are verified
2. There is no visible missing values.
3. Removed Id column from train dataset
4. All columns have meaningful names

## Convert boolean string columns to int columns
"""

## Convert object boolean columns' datatype to int (0 or 1) 
cols_lst = ['is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors', 'is_parking_camera', 'is_front_fog_lights',	'is_rear_window_wiper',	'is_rear_window_washer',	'is_rear_window_defogger',	'is_brake_assist',	'is_power_door_locks',	'is_central_locking',	'is_power_steering',	'is_driver_seat_height_adjustable'
, 'is_day_night_rear_view_mirror',	'is_ecw',	'is_speed_alert']
for col in cols_lst:
  train_df[col] = train_df[col].replace({'Yes': 1, 'No': 0})
  test_df[col] = test_df[col].replace({'Yes': 1, 'No': 0})

train_df.head()

# Verify datatypes
train_df.info()

"""## Check for columns with single value"""

train_df.columns[train_df.nunique() <= 1]

test_df.columns[test_df.nunique() <= 1]

"""There is no single column with constant values. (single unique value)

"""

train_df.head()

# 1. age_of_policyholder can't be less than policy_tenure

"""## Quick EDA using sweetviz"""

pip install sweetviz

#creating a EDA report
import sweetviz as sv
analyze_report = sv.analyze(train_df)
analyze_report.show_html('analyze.html', open_browser=True)

analyze_report = sv.analyze(test_df)
analyze_report.show_html('analyze_test.html', open_browser=True)

"""## Drop columns having high skewness"""

# List of columns to drop 
cols_lst = ['is_speed_alert','is_power_steering','is_parking_sensors']
train_df = train_df.drop(columns=cols_lst, axis = 1)
test_df = test_df.drop(columns=cols_lst, axis = 1)
print(train_df.shape)
print(test_df.shape)

"""## Columns having 2 categories """

train_df.columns[train_df.nunique() == 2]

test_df.columns[test_df.nunique() == 2]

two_uniqs = ['rear_brakes_type', 'transmission_type']
temp_train = pd.get_dummies(train_df[two_uniqs], drop_first= True)
temp_test = pd.get_dummies(test_df[two_uniqs], drop_first= True)
temp_train.head()

# Drop original columns
train_df = train_df.drop(two_uniqs,axis=1)
test_df = test_df.drop(two_uniqs,axis=1)
# concat dummy columns
train1_df = pd.concat([train_df, temp_train], axis=1)
test1_df = pd.concat([test_df, temp_test], axis=1)
train1_df.head()

# Convert datatype from numeric to object type 
train1_df = train1_df.astype({'cylinder': 'object','gear_box':'object'})
test1_df = test1_df.astype({'cylinder': 'object','gear_box':'object'})

# Create dummy df for train and test
temp_train = pd.get_dummies(train1_df[['cylinder','gear_box']], drop_first= True)
temp_test = pd.get_dummies(test1_df[['cylinder','gear_box']], drop_first= True)

# Display dummy df
temp_train.head()

# Drop original columns
train1_df = train1_df.drop(['cylinder', 'gear_box'], axis=1)
test1_df = test1_df.drop(['cylinder', 'gear_box'], axis=1)
# concat dummy columns
train2_df = pd.concat([train1_df, temp_train], axis=1)
test2_df = pd.concat([test1_df, temp_test], axis=1)
train2_df.head()

"""## Columns with more than 2 less than 5 categories (excluding 'make' column)"""

train2_df.columns[(train2_df.nunique() > 2) & (train2_df.nunique() <=5)]

temp_train = pd.get_dummies(train2_df[['fuel_type']], drop_first= True)
temp_test = pd.get_dummies(test2_df[['fuel_type']], drop_first= True)
temp_train.head()

# Drop original columns
train2_df = train2_df.drop(['fuel_type'], axis=1)
test2_df = test2_df.drop(['fuel_type'], axis=1)
# concat dummy columns
train3_df = pd.concat([train2_df, temp_train], axis=1)
test3_df = pd.concat([test2_df, temp_test], axis=1)
train3_df.head()

# Convert datatype from numeric to object type 
train3_df = train3_df.astype({'airbags': 'object'})
test3_df = test3_df.astype({'airbags': 'object'})

cols_3 = ['airbags', 'steering_type']
temp_train = pd.get_dummies(train3_df[cols_3], drop_first= False)
temp_test = pd.get_dummies(test3_df[cols_3], drop_first= False)
temp_train.head()

temp_train.drop(['airbags_1','steering_type_Manual'], axis=1, inplace=True)
temp_test.drop(['airbags_1','steering_type_Manual'], axis=1, inplace=True)
temp_train.head()

# Drop original columns
train3_df = train3_df.drop(cols_3, axis=1)
test3_df = test3_df.drop(cols_3, axis=1)

# concat dummy columns
train4_df = pd.concat([train3_df, temp_train], axis=1)
test4_df = pd.concat([test3_df, temp_test], axis=1)
train4_df.head()

"""## Columns with more than 5 categories"""

train4_df.columns[train4_df.nunique() > 5]

sorted(test4_df['gross_weight'].unique())

train4_df['gross_weight'] = train4_df['gross_weight'].replace(dict.fromkeys([1051, 1155, 1185], '<1200'))
train4_df['gross_weight'] = train4_df['gross_weight'].replace(dict.fromkeys([1335, 1340, 1410, 1490], '1300-1500'))
train4_df['gross_weight'] = train4_df['gross_weight'].replace(dict.fromkeys([1660, 1720,1510], '>1500'))
train4_df['gross_weight'].value_counts(dropna=False)

test4_df['gross_weight'] = test4_df['gross_weight'].replace(dict.fromkeys([1051, 1155, 1185], '<1200'))
test4_df['gross_weight'] = test4_df['gross_weight'].replace(dict.fromkeys([1335, 1340, 1410, 1490], '1300-1500'))
test4_df['gross_weight'] = test4_df['gross_weight'].replace(dict.fromkeys([1660, 1720,1510], '>1500'))
test4_df['gross_weight'].value_counts(dropna=False)

cols_3 = ['gross_weight']
temp_train = pd.get_dummies(train4_df[cols_3], drop_first= False)
temp_test = pd.get_dummies(test4_df[cols_3], drop_first= False)
temp_train.head()

# Drop original columns
train4_df = train4_df.drop(cols_3, axis=1)
test4_df = test4_df.drop(cols_3, axis=1)

# concat dummy columns
train5_df = pd.concat([train4_df, temp_train], axis=1)
test5_df = pd.concat([test4_df, temp_test], axis=1)
train5_df.head()

print(test5_df.shape)
print(train5_df.shape)

sorted(test5_df['width'].unique())

sorted(train5_df['width'].unique())

train5_df.head()

train5_df['segment'].value_counts(dropna=False)

train5_df['segment'] = train5_df['segment'].replace({'Utility':'Others', 'C1': 'Others', 'B1':'Others'})
test5_df['segment'] = test5_df['segment'].replace({'Utility':'Others', 'C1': 'Others', 'B1':'Others'})

train5_df['segment'].value_counts(dropna=False)

cols_3 = ['segment']
temp_train = pd.get_dummies(train5_df[cols_3], drop_first= False)
temp_test = pd.get_dummies(test5_df[cols_3], drop_first= False)
temp_train.head()

# Drop original columns
train5_df = train5_df.drop(cols_3, axis=1)
test5_df = test5_df.drop(cols_3, axis=1)

# concat dummy columns
train6_df = pd.concat([train5_df, temp_train], axis=1)
test6_df = pd.concat([test5_df, temp_test], axis=1)
train6_df.head()



train6_df['model'] = train6_df['model'].replace({'M10': "M10_11", 'M11': 'M10_11'})
test6_df['model'] = test6_df['model'].replace({'M10': "M10_11", 'M11': 'M10_11'})

train6_df['model'] = train6_df['model'].replace({'M10_11': "M7-M12", 'M7': 'M7-M12', 'M8': 'M7-M12', 'M9': 'M7-M12' })
test6_df['model'] = test6_df['model'].replace({'M10_11': "M7-M12", 'M7': 'M7-M12', 'M8': 'M7-M12', 'M9': 'M7-M12'})

train6_df['model'] = train6_df['model'].replace({'M2': "M2_3_5", 'M3': 'M2_3_5', 'M5': 'M2_3_5'})
test6_df['model'] = test6_df['model'].replace({'M2': "M2_3_5", 'M3': 'M2_3_5', 'M5': 'M2_3_5'})

train6_df['model'].value_counts(dropna=False)

cols_3 = ['model']
temp_train = pd.get_dummies(train6_df[cols_3], drop_first= False)
temp_test = pd.get_dummies(test6_df[cols_3], drop_first= False)
temp_train.head()

# Drop original columns
train6_df = train6_df.drop(cols_3, axis=1)
test6_df = test6_df.drop(cols_3, axis=1)

# concat dummy columns
train7_df = pd.concat([train6_df, temp_train], axis=1)
test7_df = pd.concat([test6_df, temp_test], axis=1)
train7_df.head()

analyze_report = sv.analyze(temp_train)
analyze_report.show_html('dum2.html', open_browser=True)

train7_df.drop(columns=['model_M2_3_5'],axis=1, inplace=True)
test7_df.drop(columns=['model_M2_3_5'],axis=1, inplace=True)
train7_df.head(3)

train7_df['engine_type'].value_counts(dropna=False)

eng_vals = ['1.2 L K12N Dualjet',
 '1.0 SCe',
 '1.5 Turbocharged Revotorq',
 '1.2 L K Series Engine',
 'K10C',
 'i-DTEC',
 'G12B',
 '1.5 Turbocharged Revotron']

train7_df['engine_type'] = train7_df['engine_type'].replace(eng_vals,'Others')
test7_df['engine_type'] = test7_df['engine_type'].replace(eng_vals,'Others')

cols_3 = ['engine_type']
temp_train = pd.get_dummies(train7_df[cols_3], drop_first= False)
temp_test = pd.get_dummies(test7_df[cols_3], drop_first= False)
temp_train.head()

# Drop original columns
train7_df = train7_df.drop(cols_3, axis=1)
test7_df = test7_df.drop(cols_3, axis=1)

# concat dummy columns
train8_df = pd.concat([train7_df, temp_train], axis=1)
test8_df = pd.concat([test7_df, temp_test], axis=1)
train8_df.head()

analyze_report = sv.analyze(temp_train)
analyze_report.show_html('dum4.html', open_browser=True)

cols_3 = ['area_cluster','max_torque','max_power']
temp_train = pd.get_dummies(train8_df[cols_3], drop_first= True)
temp_test = pd.get_dummies(test8_df[cols_3], drop_first= True)
temp_train.head()

temp_train.columns.tolist()

# Drop dumm column having high skewness
drop_cols = ['area_cluster_C10',
 'area_cluster_C11',
 'area_cluster_C12',
 'area_cluster_C13',
 'area_cluster_C14',
 'area_cluster_C15',
 'area_cluster_C16',
 'area_cluster_C17',
 'area_cluster_C18',
 'area_cluster_C19',
#  'area_cluster_C2',
 'area_cluster_C20',
 'area_cluster_C21',
 'area_cluster_C22',
#  'area_cluster_C3',
 'area_cluster_C4',
#  'area_cluster_C5',
 'area_cluster_C6',
 'area_cluster_C7',
#  'area_cluster_C8',
 'area_cluster_C9',
 'max_torque_170Nm@4000rpm',
 'max_torque_200Nm@1750rpm',
 'max_torque_200Nm@3000rpm',
#  'max_torque_250Nm@2750rpm',
#  'max_torque_60Nm@3500rpm',
 'max_torque_82.1Nm@3400rpm',
 'max_torque_85Nm@3000rpm',
 'max_torque_91Nm@4250rpm',
 'max_power_118.36bhp@5500rpm',
#  'max_power_40.36bhp@6000rpm',
 'max_power_55.92bhp@5300rpm',
 'max_power_61.68bhp@6000rpm',
 'max_power_67.06bhp@5500rpm',
#  'max_power_88.50bhp@6000rpm',
 'max_power_88.77bhp@4000rpm',
 'max_power_97.89bhp@3600rpm'
 ]

temp_train.drop(drop_cols,axis=1, inplace=True)
temp_test.drop(drop_cols,axis=1, inplace=True)

# Drop original columns
train8_df = train8_df.drop(cols_3, axis=1)
test8_df = test8_df.drop(cols_3, axis=1)

# concat dummy columns
train9_df = pd.concat([train8_df, temp_train], axis=1)
test9_df = pd.concat([test8_df, temp_test], axis=1)
train9_df.head()

"""## Standard Scaler"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

y = train9_df['is_claim']
X = train9_df.drop(['is_claim'],axis=1)

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)

# Scaling numerical columns for train dataset
scaler=StandardScaler()
num_cols = X_train.select_dtypes(include=['float64','int64','uint8']).columns
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_train.head()

X_test[num_cols] = scaler.transform(X_test[num_cols])
X_test.head()

"""## Quick Model building"""

# from xgboost import XGBClassifier

import lightgbm as lgb
lgbm = lgb.LGBMClassifier(objective='binary', random_state=1, n_jobs=-1)
lgbm.fit(X_train, y_train)

y_pred = lgbm.predict(X_test)

from sklearn.metrics import classification_report

cr = classification_report(y_test, y_pred)
# cm = confusion_matrix(y_test, y_pred)
 
print(cr)

from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.metrics import balanced_accuracy_score
brf = BalancedRandomForestClassifier(n_estimators=100, random_state=0)
brf.fit(X_train, y_train)
y_pred = brf.predict(X_test)
print("Accuracy",balanced_accuracy_score(y_test, y_pred))

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average='macro')



"""## Predict on test data"""



X_eval = test9_df.drop(['policy_id'], axis=1)

X_eval[num_cols] = scaler.transform(X_eval[num_cols])
X_eval.head()

y_dum = brf.predict(X_train)
print("Accuracy",balanced_accuracy_score(y_train, y_dum))

f1_score(y_train, y_dum, average='macro')

y_eval = brf.predict(X_eval)
# print("Accuracy",balanced_accuracy_score(y_test, y_pred))

sub_df = pd.read_csv("/content/HackVerse/sample_submission_KvRh9Sx.csv")
sub_df.head()

sub_df['is_claim'] = y_eval

sub_df['is_claim'].value_counts(dropna=False)

sub_df.to_csv("Result1.csv")



"""## Imbalanced dataset considering target column"""

train_df['is_claim'].value_counts(dropna=False)

# from imblearn.over_sampling import SMOTE

# # Resample the minority class. You can change the strategy to 'auto' if you are not sure.
# sm = SMOTE(sampling_strategy='minority', random_state=7)

# # Fit the model to generate the data.
# oversampled_trainX, oversampled_trainY = sm.fit_sample(train9_df.drop('is_claim', axis=1), train9_df['is_claim'])
# oversampled_train = pd.concat([pd.DataFrame(oversampled_trainY), pd.DataFrame(oversampled_trainX)], axis=1)
# oversampled_train.columns = normalized_df.columns

